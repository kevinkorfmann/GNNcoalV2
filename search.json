[
  {
    "objectID": "gnncoal.html",
    "href": "gnncoal.html",
    "title": "GNNcoalV2",
    "section": "",
    "text": "source\n\nGNN\n\n GNN (in_channels, hidden_channels, out_channels, normalize=True,\n      lin=True, track_running_stats=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDiffPoolNet\n\n DiffPoolNet (max_nodes, num_features, num_hidden=64, out_channels=60,\n              track_running_stats=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMiniDenseNet\n\n MiniDenseNet (in_features, hidden_features, out_features)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDiffPoolNetV2\n\n DiffPoolNetV2 (max_nodes, num_features, num_hidden=64, out_channels=60,\n                num_trees=500, enc_depth=12, enc_heads=8,\n                track_running_stats=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDiffPoolNetV3\n\n DiffPoolNetV3 (max_nodes, num_features, num_hidden=64, out_channels=60,\n                num_trees=500, enc_depth=12, enc_heads=8,\n                track_running_stats=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "data_generation.html",
    "href": "data_generation.html",
    "title": "GNNcoalV2",
    "section": "",
    "text": "source\n\nseed_everything\n\n seed_everything (seed=42)\n\nSeed all random number generators.\n\nsource\n\n\nsample_constant_population_size\n\n sample_constant_population_size (n_min:int=10, n_max:int=100000,\n                                  num_time_windows=21)\n\n\nsource\n\n\nsample_population_size\n\n sample_population_size (n_min:int=10, n_max:int=100000,\n                         num_time_windows=21)\n\nCreates random demography. Function taken from: https://gitlab.inria.fr/ml_genetics/public/dlpopsize_paper\n:param int n_min: Lower-bound of demography. :param int n_max: Upper-bound of demography. :param int num_time_windows: Number of population sizes in demography. :return list:\n\nsource\n\n\nget_population_time\n\n get_population_time (time_rate:float=0.06, tmax:int=130000,\n                      num_time_windows:int=21)\n\nCreates population time points; used as time points to change population size changes for simulation\n:return numpy.ndarray: time points of length num_time_windows\n\nsource\n\n\nsimulate_scenario\n\n simulate_scenario (population_size:Union[list,numpy.ndarray],\n                    population_time:Union[list,numpy.ndarray],\n                    mutation_rate:float, recombination_rate:float,\n                    segment_length:float, num_sample:int,\n                    num_replicates:int, seed:int=69420, model=None)\n\nSimulates tree sequence with msprime given population size changes at specific time-points. Piece-wise constant simualtion of demography.\n:return: generator of tskit.trees.TreeSequence\n\nsource\n\n\nsample_parameters\n\n sample_parameters (num_time_windows=60, n_min=10000, n_max=10000000,\n                    recombination_rates=[1e-08, 1e-08],\n                    population_size=None, model=None)\n\n\nsource\n\n\nsample_population_size\n\n sample_population_size (n_min:int=10, n_max:int=100000,\n                         num_time_windows=21)\n\nCreates random demography. Function taken from: https://gitlab.inria.fr/ml_genetics/public/dlpopsize_paper\n:param int n_min: Lower-bound of demography. :param int n_max: Upper-bound of demography. :param int num_time_windows: Number of population sizes in demography. :return list:\n\nsource\n\n\nsample_smooth_population_parameters\n\n sample_smooth_population_parameters ()\n\n\nsource\n\n\nsimulate_tree_sequence\n\n simulate_tree_sequence (parameters:pandas.core.frame.DataFrame,\n                         population_time:list, segment_length=1000000.0,\n                         num_sample=10, num_replicates=100, seed=69420)\n\n\nsource\n\n\ngenerate_sample\n\n generate_sample (nth_scenario, num_sim_trees, alpha)\n\n\nsource\n\n\nalternative_coalescent_mask\n\n alternative_coalescent_mask (ts, population_time, threshold=10,\n                              num_sim_trees=500, nth_tree=1)\n\n\nsource\n\n\nmask_smaller_fragments\n\n mask_smaller_fragments (arr)\n\n\nsource\n\n\nts_to_data_objects\n\n ts_to_data_objects (ts, num_sim_trees=500, nth_tree=1, num_embedding=60,\n                     y=None)\n\n\nsource\n\n\ncalculate_beta_coal_ne_estimate\n\n calculate_beta_coal_ne_estimate (theta, sample_size, L, alpha, mu_real)\n\n\nsource\n\n\ngenerate_batch\n\n generate_batch (seed, alpha, batch_size, num_trees, num_sim_trees,\n                 nth_tree=1, scaling='log', mutation_rate=4e-08)\n\n\nsource\n\n\ncontinously_add_batch_to_queue\n\n continously_add_batch_to_queue (data_queue, process_id, num_processes,\n                                 seed_start_value, num_trees=1000,\n                                 num_sim_trees=1000, nth_tree=1,\n                                 batch_size=4, scaling='log',\n                                 mutation_rate=4e-08)\n\n\nsource\n\n\nget_next_batch\n\n get_next_batch (data_queue)\n\n\n#np.random.seed(0x1337)\n\n\n# tss, demographies, alphas = [], [], []\n# for i in range(1):\n#     ts, demography, alpha = generate_sample(i)\n#     tss.append(ts)\n#     demographies.append(demography)\n#     alphas.append(alpha)\n# print(alphas)\n\n\n#population_time, _ = sample_smooth_population_parameters()\n\n\n#alternative_coalescent_mask(ts, population_time)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GNNcoalV2",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "GNNcoalV2",
    "section": "Install",
    "text": "Install\npip install GNNcoalV2"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "GNNcoalV2",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "gnncoalv2.html",
    "href": "gnncoalv2.html",
    "title": "GNNcoalV2",
    "section": "",
    "text": "source\n\nseed_everything\n\n seed_everything (seed=42)\n\nSeed all random number generators.\n\nsource\n\n\nGNN\n\n GNN (in_channels, hidden_channels, out_channels, linearize=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# #| export\n\n# class GNN(nn.Module):\n#     def __init__(self, in_channels, hidden_channels, out_channels, linearize=True):\n#         super(GNN, self).__init__()\n\n#         self.convs = nn.ModuleList([\n#             DenseGCNConv(in_channels, hidden_channels, improved=True),\n#             DenseGCNConv(hidden_channels, out_channels, improved=True)\n#         ])\n\n#         self.norms = nn.ModuleList([\n#             nn.LayerNorm(hidden_channels),\n#             nn.LayerNorm(out_channels)\n#         ])\n\n#         self.linearize = linearize\n#         if self.linearize: self.linear = nn.Linear(hidden_channels + out_channels, out_channels)\n#         else: self.linear = None\n        \n\n#     def forward(self, x, adj):\n#         xs = []\n#         for conv, norm in zip(self.convs, self.norms):\n#             x = F.relu(conv(x, adj))\n#             x = norm(x)\n#             xs.append(x)\n#         x = torch.cat(xs, dim=-1)\n        \n#         if self.linearize: return F.relu(self.linear(x))\n#         else: return x\n\n\nsource\n\n\nDiffPoolConvolutionPart\n\n DiffPoolConvolutionPart (max_nodes, num_features, num_hidden=64)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMiniDenseNet\n\n MiniDenseNet (in_features, hidden_features, out_features)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nmean_over_n\n\n mean_over_n (emb, n=10)\n\n\nsource\n\n\nGNNcoalV2\n\n GNNcoalV2 (max_nodes, num_features, num_hidden=64, out_dim=60,\n            enc_heads=4, enc_depth=8, num_trees=1000)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n#sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n\n\n# import msprime\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n\n# tss = []\n\n# for i in range(32):\n#     ts = msprime.sim_ancestry(samples=10, population_size=10_000,\n#                               sequence_length=2e6, ploidy=1, recombination_rate=1e-8)\n#     tss.append(ts)\n\n\n# data_objects = []\n# for ts in tss:\n#     i_data_object = ts_to_data_objects(ts)\n#     data_objects += i_data_object\n\n\n# import torch\n# import networkx as nx\n# from torch_geometric.loader import DataLoader\n# from torch_geometric.utils.convert import from_networkx\n# from torch_geometric.utils.to_dense_adj import to_dense_adj\n# from torch_geometric.utils.to_dense_batch import to_dense_batch\n\n\n# bs = 8\n# dl = DataLoader(data_objects, batch_size=500*bs)\n# data_batch = next(iter(dl))\n# batch = data_batch\n# adj = to_dense_adj(batch.edge_index, batch=batch.batch, edge_attr=batch.edge_weight, max_num_nodes=None)\n# x, _ = to_dense_batch(batch.x, batch.batch)\n\n\n#demography, alpha_values, l_total, e_total = model(x, adj, batch_size=8)"
  }
]